# BioinfoFlow Specification v0.1 ğŸ§¬

## Table of Contents
- [Overview](#overview) ğŸ“‹
- [Core Concepts](#core-concepts) ğŸ’¡
- [Configuration Reference](#configuration-reference) âš™ï¸
- [Examples](#examples) ğŸŒŸ
- [Key Features](#key-features) â­
- [Best Practices](#best-practices) ğŸ¯
- [Style Guide](#style-guide) ğŸ“
- [Version Control](#version-control) ğŸ”„

## Overview ğŸ“‹

BioinfoFlow is a powerful workflow engine specifically designed for bioinformatics applications, enabling reproducible, scalable, and container-native data analysis pipelines. This specification defines a human-readable workflow language that emphasizes simplicity and maintainability.

### Target Audience
- Bioinformaticians and computational biologists
- Research laboratories and institutions
- Bioinformatics pipeline developers
- Data scientists working with biological data

### Use Cases
- High-throughput sequencing data analysis
- Genomics and transcriptomics workflows
- Population genetics studies
- Clinical genomics pipelines
- Custom bioinformatics tool integration

## Core Concepts ğŸ’¡

BioinfoFlow is built around four fundamental concepts that work together to create robust bioinformatics pipelines:

### Workflows
A workflow is a collection of steps that process biological data. Each workflow:
- Has a unique name and version
- Contains one or more steps
- Defines its input requirements
- Specifies resource requirements
- Maintains reproducibility through containerization

### Steps
Steps are the basic building blocks of a workflow. They can be:
- Single steps (e.g., quality control, alignment)
- Parallel steps (e.g., sample-level processing)
- Sequential steps (e.g., variant calling pipeline)

Each step is:
- Containerized for reproducibility
- Resource-aware for efficient execution
- Input/output tracked for dependency management
- Automatically logged for monitoring

### Dependencies
Steps can depend on other steps, forming a directed acyclic graph (DAG):
- Explicit dependencies via 'after' field
- Implicit dependencies through input/output relationships
- Parallel execution of independent steps
- Automatic dependency resolution

### Containers
Each step runs in its own container environment, ensuring:
- Software version control
- Environment isolation
- Reproducibility across systems
- Portable execution

## Configuration Reference âš™ï¸

BioinfoFlow uses YAML format for workflow definitions. This section details the configuration structure and options available.

### Root Level Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| name | string | Yes | Workflow name |
| version | string | Yes | Workflow version |
| description | string | No | Workflow description |
| global | object | No | Global configuration |
| inputs | object | No | Global inputs |
| steps | object | Yes | Workflow steps |
| resources | object | No | Resource defaults |

### Global Configuration

The `global` section defines both environmen
t variables and configuration settings:

```yaml
global:
  # Directory paths
  base_dir: "/path/to/workflows"
  # work_dir: "/path/to/workflows/${workflow.name}" # workflow instance directory, default to workflow name
  # tmp_dir: "/path/to/workflows/tmp" # temporary files directory, default to workflows base dir

  # Runtime settings
  max_retries: 3

  # Reference data paths
  ref_genome: "reference/hg38.fa" # Path relative to base_dir
  dbsnp: "reference/dbsnp.vcf.gz" # Path relative to base_dir
  known_indels: "reference/known_indels.vcf.gz" # Path relative to base_dir
```

### Directory Structure

All workflow files are organized under the `work_dir`:
```
/tasks                      # work_dir
â”œâ”€â”€ tmp/                   # Temporary files
â”œâ”€â”€ reference/             # Reference data
â””â”€â”€ workflow_name/         # Workflow instance directory
    â”œâ”€â”€ 202403201234_xxxx/  # run_id (timestamp + uuid)
    â”‚   â”œâ”€â”€ inputs/       # Task input files
    â”‚   â”œâ”€â”€ outputs/      # Task output files
    â”‚   â””â”€â”€ logs/         # Task logs
    â””â”€â”€ 202403201235_yyyy/
        â”œâ”€â”€ inputs/
        â”œâ”€â”€ outputs/
        â””â”€â”€ logs/
```

### Input Configuration

The `inputs` section supports both single files and sample groups:

```yaml
inputs:
  # Single file input
  reads:
    type: file
    pattern: "*.fastq.gz"
  
  # Sample group input
  samples:
    type: group
    pattern: "samples.csv"    # Sample information file
    format: csv
    columns:
      - name: sample_id
        type: string
        unique: true
      - name: read1
        type: file
        pattern: "*.fastq.gz"
      - name: read2
        type: file
        pattern: "*.fastq.gz"
```

### Step Configuration

Each step follows a consistent structure:

```yaml
steps:
  step_name:
    container: "image:tag"              # Container image
    foreach: sample in samples          # Optional: iterate over samples
    inputs:
      input1: ${steps.prev.outputs.out} # Reference to previous step output
      input2: ${global.some_config}        # Reference to environment
    outputs:
      output1: "path/to/output"         # Output definition
    command: "command ${inputs.input1}" # Command template
    after: [dependency1, dependency2]    # Step dependencies
    volumes:                            # Volume configuration
      - /host/path:/container/path         # Host source path
    resources:                          # Resource requirements
      cpu: 2
      memory: 4G
```

Key Fields:
- `container`: Container image reference
- `foreach`: Optional field for sample iteration
- `inputs`: Input definitions with references
- `outputs`: Output path definitions (using task directory)
- `command`: Command template with variable substitution
- `after`: Step dependencies
- `volumes`: Container volume configuration
- `resources`: Resource requirements

### Variable References

Variables can be referenced using `${...}` syntax:
- `${global.VAR}`: Global reference
- `${inputs.NAME}`: Input reference
- `${steps.STEP.outputs.NAME}`: Step output reference
- `${sample.FIELD}`: Sample field reference (in foreach context)
- `${resources.NAME}`: Resource reference
- `${workflow.name}`: Current workflow name
- `${step.name}`: Current step name

### Multi-sample Processing

For workflows processing multiple samples:
1. Define sample structure in `inputs.samples`
2. Use `foreach: sample in samples` in steps
3. Reference sample fields with `${sample.field_name}`
4. Collect outputs using step references

## Examples ğŸŒŸ

BioinfoFlow examples demonstrate common bioinformatics workflows and best practices for configuration.

### Variant Calling Pipeline Example

This example shows a complete germline variant calling workflow using BWA and GATK4:

```yaml
name: variant_calling_pipeline
version: "1.0.0"
description: "Germline variant calling workflow using BWA and GATK4"

global:
  ref_genome: "reference/hg38.fa"
  dbsnp: "reference/dbsnp.vcf.gz"
  known_indels: "reference/known_indels.vcf.gz"
  tmp_dir: "/tmp"
  max_retries: 3

inputs:
  samples:
    type: group
    pattern: "samples.csv"
    format: csv
    columns:
      - name: sample_id
        type: string
        unique: true
      - name: read1
        type: file
        pattern: "*.fastq.gz"
      - name: read2
        type: file
        pattern: "*.fastq.gz"

steps:
  fastqc:
    container: "biocontainers/fastqc:v0.11.9"
    volumes:
      - ${global.tmp_dir}:/tmp
    foreach: sample in samples
    inputs:
      read1: ${sample.read1}
      read2: ${sample.read2}
    outputs:
      qc_report: "qc/${sample.sample_id}/fastqc_report.html"
    command: |
      fastqc ${inputs.read1} ${inputs.read2} -o $(dirname ${outputs.qc_report})
    resources:
      cpu: 2
      memory: 4G

  bwa_mem:
    container: "biocontainers/bwa:v0.7.17"
    volumes:
      - ${global.tmp_dir}:/tmp
    foreach: sample in samples
    inputs:
      read1: ${sample.read1}
      read2: ${sample.read2}
      ref: ${global.ref_genome}
    outputs:
      bam: "aligned/${sample.sample_id}.bam"
    command: |
      bwa mem -t ${resources.cpu} \
      -R "@RG\tID:${sample.sample_id}\tSM:${sample.sample_id}" \
      ${inputs.ref} ${inputs.read1} ${inputs.read2} \
      | samtools sort -@ ${resources.cpu} -o ${outputs.bam}
    after: [fastqc]
    resources:
      cpu: 8
      memory: 16G

  variant_call:
    container: "broadinstitute/gatk:4.2.6.1"
    volumes:
      - ${global.tmp_dir}:/tmp
    foreach: sample in samples
    inputs:
      bam: ${steps.bwa_mem.outputs.bam}
      ref: ${global.ref_genome}
    outputs:
      vcf: "variants/${sample.sample_id}.vcf.gz"
    command: |
      gatk HaplotypeCaller \
      -I ${inputs.bam} \
      -R ${inputs.ref} \
      -O ${outputs.vcf}
    after: [bwa_mem]
    resources:
      cpu: 4
      memory: 16G
```

## Experimental Features ğŸ§ª

### Metadata
```yaml
metadata:
  author: name
  tags: [DNA-seq, RNA-seq]
  license: MIT
  documentation: "https://docs.example.com"
```

### Conditions
```yaml
conditions:
  file_exists:
    when: "exists:/path/to/file"
    skip: false
```

### Hooks
```yaml
hooks:
  before_step:            # Before step execution
    - name: string
      script: string
  after_step:             # After step execution
    - name: string
      script: string
  on_success:             # On workflow success
    - name: string
      script: string
  on_failure:             # On workflow failure
    - name: string
      script: string
```

### Notifications
```yaml
notifications:
  email:
    recipients: ["user@example.com"]
    subject: "Workflow completed"
    body: "Workflow completed successfully"
  slack:
    webhook_url: "https://hooks.slack.com/services/.../..."
  discord:
    webhook_url: "https://discord.com/api/webhooks/.../..."
  feishu:
    webhook_url: "https://open.larkoffice.com/open-apis/bot/v2/hook/.../..."
```

## Key Features â­

- ğŸ³ Container-native execution with automatic version management
- ğŸ“Š Flexible resource management
- ğŸ’¾ Built-in checkpoint and resume capability
- ğŸ” Dynamic input pattern matching
- âš¡ Parallel and sequential execution support
- ğŸ”— Clear dependency management
- ğŸ›¡ï¸ Environment isolation

## Best Practices ğŸ¯

Follow these guidelines to create efficient and maintainable workflows:

### Container Management ğŸ³
- Use specific version tags for reproducibility
- Choose official or verified container images
- Keep container images minimal and focused

### Resource Management ğŸ“Š
- Set appropriate CPU and memory limits
- Use resource profiles for different environments
- Monitor resource usage patterns

### Workflow Design ğŸ“
- Keep steps modular and focused
- Use meaningful step and variable names
- Document inputs, outputs, and dependencies
- Follow the principle of least privilege

### Error Handling âš ï¸
- Implement appropriate error checking
- Use retry strategies for transient failures
- Log errors with sufficient context
- Clean up temporary resources

## Style Guide ğŸ“

Follow these conventions for consistent workflow definitions:

### Naming Conventions
- Use lowercase for workflow names
- Use snake_case for step names
- Use descriptive names for inputs and outputs

### YAML Formatting
- Use 2 spaces for indentation
- Break long commands with line continuations
- Group related configuration items

### Value Formatting
Use quotes for:
- File paths: `"path/to/file"`
- Version numbers: `"1.0.0"`
- Commands with variables
- Container references

Don't quote:
- Numbers
- Resource units
- Boolean values
- Simple identifiers

## Version Control ğŸ”„

This is version 0.1 of the BioinfoFlow specification. Future versions will focus on:
- Enhanced container orchestration
- Advanced dependency management
- Extended security features
- Improved monitoring capabilities 